{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a11623-6c33-44ae-ae43-7e2adac269ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "st.set_page_config(page_title=\"ARQM Quality Check\", page_icon=\"üß±\")\n",
    "\n",
    "# --- 2. LOAD YOUR AI MODEL ---\n",
    "# We use @st.cache_resource so the model loads once and stays in memory (faster)\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    # REPLACE 'best.pt' with the actual path to your trained model file\n",
    "    # If you don't have the file yet, this line will fail.\n",
    "    model = YOLO('best.pt') \n",
    "    return model\n",
    "\n",
    "try:\n",
    "    model = load_model()\n",
    "    model_loaded = True\n",
    "except Exception as e:\n",
    "    st.error(f\"Model not found! Make sure your 'best.pt' file is in the same folder.\")\n",
    "    model_loaded = False\n",
    "\n",
    "# --- 3. THE APP INTERFACE ---\n",
    "st.title(\"ARQM: Site Quality Check\")\n",
    "st.write(\"Take a photo of the installation to verify compliance.\")\n",
    "\n",
    "# This creates the camera widget\n",
    "img_file_buffer = st.camera_input(\"Capture Installation\")\n",
    "\n",
    "# --- 4. THE MAGIC (INFERENCE) ---\n",
    "if img_file_buffer is not None and model_loaded:\n",
    "    # Convert the photo from Streamlit format to an OpenCV format\n",
    "    bytes_data = img_file_buffer.getvalue()\n",
    "    cv2_img = cv2.imdecode(np.frombuffer(bytes_data, np.uint8), cv2.IMREAD_COLOR)\n",
    "\n",
    "    # Run the AI Model on the image\n",
    "    results = model(cv2_img)\n",
    "\n",
    "    # --- 5. PROCESS RESULTS ---\n",
    "    # We look at the first result (since we only sent one image)\n",
    "    result = results[0]\n",
    "    \n",
    "    # Draw the boxes on the image\n",
    "    # result.plot() automatically draws the boxes and labels (e.g., \"Bent\", \"Gap\")\n",
    "    annotated_frame = result.plot()\n",
    "\n",
    "    # Convert color back to RGB for display (OpenCV uses BGR)\n",
    "    annotated_frame_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # --- 6. DISPLAY VERDICT ---\n",
    "    st.image(annotated_frame_rgb, caption=\"AI Analysis Result\")\n",
    "\n",
    "    # Optional: Simple Text Verdict Logic\n",
    "    # Check if the model detected any specific classes (assuming class 0 is 'Bent')\n",
    "    boxes = result.boxes\n",
    "    if len(boxes) > 0:\n",
    "        # Example: If your model is trained to detect \"Defects\"\n",
    "        st.error(\"‚ùå INCORRECT INSTALLATION DETECTED\")\n",
    "        st.write(\"Please check the highlighted area.\")\n",
    "    else:\n",
    "        # If no boxes were found (or if your model detects 'Correct' as a class)\n",
    "        st.success(\"‚úÖ INSTALLATION PASSED\")\n",
    "        st.write(\"No defects detected. Audit record created.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
